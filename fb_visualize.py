# -*- coding: utf-8 -*-
"""fb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pji-n9p4kQHWccy6tCGOeBPI7Fz7Uvqn
"""

# Commented out IPython magic to ensure Python compatibility.
# #import library
# %%capture
# # dev version
# !pip install https://github.com/PyThaiNLP/pythainlp/archive/dev.zip
# 
# # release version 
# ! pip install pythainlp
# 
# !pip install epitran
# !pip install sklearn_crfsuite
# !pip install tensorflow deepcut
# !pip install attacut
# !pip install emoji
# import tweepy
# import pandas as pd
# from sklearn.feature_extraction.text import CountVectorizer
# import numpy as np
# import emoji
# from pythainlp.tokenize import word_tokenize
# from pythainlp.corpus import thai_stopwords
# import re
# from wordcloud import WordCloud
# import matplotlib.pyplot as plt

import pandas as pd
import numpy as np
df4 = pd.read_csv('/content/drive/MyDrive/visualize project/Twitter_Data.csv')
df4.info()

#twitter chart 
testtime = df4[['created_at','retweet']]
time = pd.DatetimeIndex(testtime.created_at)
grouped = testtime.groupby([time.hour]).retweet.sum()
dfgrouped = grouped.to_frame()
dfgrouped.info()
defgrouped = {'time':list(range(0,24)),
              'retweet':dfgrouped['retweet']}
dfnewtime = pd.DataFrame(defgrouped)
dfnewtime

#twitter bar chart 
import altair as alt

bar = alt.Chart(dfnewtime).mark_bar().encode(
    x='time:O',
    y='retweet:Q'
)
rule = alt.Chart(dfnewtime).mark_rule(color='red').encode(
    y='mean(retweet):Q'
)
(bar + rule).properties(width=600)

#twitter line chart
import altair as alt
line =alt.Chart(dfnewtime).mark_line().encode(
    x='time',
    y='retweet'
).properties(width=800)
line

#twitter explore
import pandas as pd
import numpy as np
df4 = pd.read_csv('/content/drive/MyDrive/visualize project/Twitter_Data.csv')
df4.info()

#ลองนับความยาว content ใน twitter
dfcutspace = pd.DataFrame(df4['message'])

cutspace = []
for i in range(0,len(dfcutspace['message'])):
  cutspace.append(dfcutspace['message'][i].replace(" ",""))

dfcut = pd.DataFrame({'messagecl':cutspace})

cutn = []
for i in range(0,len(dfcut['messagecl'])):
  cutn.append(dfcut['messagecl'][i].replace("\n",""))
dfcut = pd.DataFrame({'messagecl':cutn})

dftwit1 = pd.DataFrame(df4)
dftwit2 = pd.concat([dftwit1,dfcut],axis=1)

sumlen = []
for i in range(0,len(dftwit2['messagecl'])):
  sumlen.append(len(dftwit2['messagecl'][i]))
dfsumlen = pd.DataFrame({'sumlen':sumlen})
dftwit2 = pd.concat([dftwit2,dfsumlen],axis=1)
dftwit2.head()

#ลองทำ Heatmap
dftwit2['created_at'] = pd.to_datetime(dftwit2['created_at'])
dftwit2['day'] = dftwit2['created_at'].dt.day_name()
dftwit2['hours'] = dftwit2['created_at'].dt.hour
dftwit2.head()

#heatmap twitter จำนวนการ retweet ในแต่ละช่วงเวลา
daynameindex = ['Sunday', 'Monday', 'Tuesday','Wednesday',"Thursday",'Friday' ,'Saturday' ]
data2 = pd.DataFrame({'day' : dftwit2['created_at'].dt.day_name().astype('str'), 'hours' : dftwit2['created_at'].dt.hour.astype('int'), 'retweet' : dftwit2['retweet']})
hours = data2['hours'].unique()
day = list({d for d in dftwit2['day']})
dataUsing = pd.DataFrame({'day' : [], 'hours' : [], 'average' : []})

for i in day :
  for j in hours :
    new_row = {'day' : i, 'hours' : j, 'average' : float(data2[(data2['day'] == i) & (data2['hours'] == j)][['retweet']].sum())}
    dataUsing = dataUsing.append(new_row, ignore_index=True)
dataUsing['hours'] = dataUsing['hours'].astype("int")
chart = alt.Chart(dataUsing).mark_rect().encode(
    x='hours:O',
    y=alt.Y('day:O', sort = daynameindex),
    color=alt.Color('average:Q', title = "")
)
chart.properties(title = "sum of retweet")

#prepare twitter data
twitengage = dftwit2[['engagement','sumlen']].groupby(['sumlen']).engagement.sum()
col1 = set(dftwit2['sumlen'])
twitengage1 = pd.DataFrame({'sumlen':list(col1),'engagement':twitengage})
twitengage1.iloc[0:30]

#line chart  จำนวนความยาวของ message ต่อ engagement
import altair as alt


line2 =alt.Chart(twitengage1).mark_line().encode(
    x=alt.X('sumlen',title = 'sum string'),
    y='engagement'
)
line2.properties(width=1200)

#Facebook Data
import pandas as pd
import numpy as np

dffbq1 = pd.read_csv('/content/drive/MyDrive/visualize project/Facebook_Data_Q1.csv')
dffbq2 = pd.read_csv('/content/drive/MyDrive/visualize project/Facebook_Data_Q2.csv')
dffbq3 = pd.read_csv('/content/drive/MyDrive/visualize project/Facebook_Data_Q3.csv')
dffbq4 = pd.read_csv('/content/drive/MyDrive/visualize project/Facebook_Data_Q4.csv')

#Group all FB Data
dffb= pd.concat([dffbq1,dffbq2,dffbq3,dffbq4])
dffb.info()

#cleaning FB
null = dffbq1['category'][0]
dffb['message'] = dffb['message'].replace(null, str('(รูปภาพ/วิดีโอ)'))
dffb['message'] = dffb['message'].replace(np.nan, str('(รูปภาพ/วิดีโอ)'))
for i in dffb.columns :
  dffb[i] = dffb[i].replace(null, np.nan)
dffb['created_at']=pd.to_datetime(dffb['created_at'])
dffb.loc[(dffb['created_at'] >= '2020-02-19')& (dffb['created_at'] < '2020-02-20')].sort_values(by='share',ascending=False).head(20)

#Top engagement FB
fbengage = dffb[['account_display_name','like','love','wow','haha','sad','angry','reaction','comment','share','engagement','is_photo']]
fbsum = fbengage.groupby(by='account_display_name').sum()
a = fbsum.sort_values(by='engagement',ascending=False).head(20)
a = a.reset_index()
a

#Prepare Data for stacked bar chart
df1 = a[['account_display_name','like']]
df1 =df1.rename(columns={'account_display_name':'account_display_name','like':'engage'})
df1['type'] = str('like')
df2 = a[['account_display_name','love']]
df2 =df2.rename(columns={'account_display_name':'account_display_name','love':'engage'})
df2['type'] = str('love')
df3 = a[['account_display_name','wow']]
df3 =df3.rename(columns={'account_display_name':'account_display_name','wow':'engage'})
df3['type'] = str('wow')
df4 = a[['account_display_name','haha']]
df4 =df4.rename(columns={'account_display_name':'account_display_name','haha':'engage'})
df4['type'] = str('haha')
df5 = a[['account_display_name','sad']]
df5 =df5.rename(columns={'account_display_name':'account_display_name','sad':'engage'})
df5['type'] = str('sad')
df6 = a[['account_display_name','angry']]
df6 =df6.rename(columns={'account_display_name':'account_display_name','angry':'engage'})
df6['type'] = str('angry')
dfnew1= df1.append([df2,df3,df4,df5,df6])
dfnew2 = df2.append([df3,df4,df5,df6])
dfnew1

#FB stacked bar chart (include like)
import altair as alt
from vega_datasets import data

source = dfnew1
# print(source)
alt.Chart(source).mark_bar().encode(
    x='sum(engage)',
    y=alt.Y('account_display_name:N',sort = '-x'),
    color='type'
    
).properties(width=800,height=500)

#FB stacked bar chart (not include like)
import altair as alt
from vega_datasets import data

source = dfnew2
# print(source)
character = alt.Chart(source,title = 'Character Facebook Stacked Bar Chart ').mark_bar().encode(
    x=alt.X('sum(engage)',title='Engagement'),
    y=alt.Y('account_display_name:N',sort = '-x',title = 'Top 20 Facebook Page'),
    color='type'
    
).properties(width=600,height=500).interactive()
character.save('character Facebook_stackedchart.html')
character

#FB stacked bar Normalized chart (not include like) 
import altair as alt
from vega_datasets import data

source = dfnew2
# print(source)
character = alt.Chart(source).mark_bar().encode(
    x=alt.X('sum(engage)',stack="normalize"),
    y=alt.Y('account_display_name:N',sort = '-x'),
    color='type'
).properties(width=800,height=500).interactive()
character.save('characterchart2.html')
character

#prepare Data For stream chart
a = dffb
a= a[['created_at','love','wow','haha','sad','angry']]
b = a.groupby(a['created_at'].dt.date).sum()
b = b.reset_index()
b.drop(b.index[363],axis=0,inplace=True)
b.drop(b.index[90],axis=0,inplace=True)
c = b.columns.to_list()
c.remove('created_at')
c
# b.tail(5)
# b.head(5)
# b['created_at'].dt.date
# c = b.columns.to_list()
# colnames = c.remove('created_at')
newdf = pd.DataFrame()
newdf2 = pd.DataFrame()

# for j in range(6):
for i in c :
    newdf = b[['created_at',str(i)]]
    newdf = newdf.drop(columns=str(i))
    newdf['count'] = b[str(i)] 
    newdf['series'] = str(i)
    newdf2 = newdf2.append(newdf)
newdf2.sort_values(by='created_at').reset_index()
newdf2['created_at'] = pd.to_datetime(newdf2['created_at'])
newdf2 = newdf2.reset_index()
newdf2 = newdf2.drop(columns='index')
newdf2

#stream chart reaction
import altair as alt
from vega_datasets import data

source = newdf2



alt.Chart(source).mark_area().encode(
    alt.X('monthdate(created_at):T',
        axis=alt.Axis(format='%Y', domain=False, tickSize=0)
    ),
    alt.Y('sum(count):Q', stack='center', axis=None),
    alt.Color('series:N',
        scale=alt.Scale(scheme='category20b')
    )
).properties(width=800,height=500).interactive()

#stream chart reaction
import altair as alt
from vega_datasets import data

source = newdf2



alt.Chart(source).mark_area().encode(
    alt.X('monthdate(created_at):T',
        axis=alt.Axis(format='%m', domain=False, tickSize=0)
    ),
    alt.Y('sum(count):Q', stack='center', axis=None),
    alt.Color('series:N')
    ).properties(width=800,height=500).interactive()

#check top reaction and top message
sadtop = newdf2[newdf2['series']=='sad']
sadtop = sadtop.sort_values(by='count',ascending=False)
sadtop.head(20)
test = dffb.loc[(dffb['created_at'] >= '2020-01-10')& (dffb['created_at'] <= '2020-01-11')]
test = test[['account_display_name','created_at','message','sad','comment','share','engagement']].sort_values(by='sad',ascending=False)
test = test.head(500)
test

#prepare data for heatmap and stacked area chart
a = dffb
a= a[['created_at','love','haha','sad','angry','wow']]
b = a.groupby(a['created_at'].dt.date).sum()
b = b.reset_index()
b.drop(b.index[363],axis=0,inplace=True)
b.drop(b.index[90],axis=0,inplace=True)

c = b.columns.to_list()
c.remove('created_at')
c
# b.tail(5)
# b.head(5)
# b['created_at'].dt.date
# c = b.columns.to_list()
# colnames = c.remove('created_at')
newdf = pd.DataFrame()
newdf2 = pd.DataFrame()

# for j in range(6):
for i in c :
    newdf = b[['created_at',str(i)]]
    newdf = newdf.drop(columns=str(i))
    newdf['count'] = b[str(i)] 
    newdf['series'] = str(i)
    newdf2 = newdf2.append(newdf)
    
newdf2.sort_values(by='created_at').reset_index()
newdf2['created_at'] = pd.to_datetime(newdf2['created_at'])
newdf2['month'] = newdf2['created_at'].dt.strftime('%B')
newdf2 = newdf2.reset_index()
newdf2 = newdf2.drop(columns=['index'])  
newdf2
# newdf2.loc[(newdf2['created_at'] >= '2020-02-09')& (newdf2['created_at'] < '2020-02-10')]

#set top message for heatmap
from datetime import timedelta

message = {'created_at':[],'series':[],'Top_post':[],'N':[]}
for i in newdf2.index:
    a = newdf2['created_at'][i]
    b = newdf2['created_at'][i]+timedelta(days=1)
    c = newdf2['series'][i]
    test = dffb.loc[(dffb['created_at'] >= a ) & (dffb['created_at'] < b)]
    test = test[['account_display_name','created_at','message',c,'comment','share','engagement']].sort_values(by=c,ascending=False)
    d = test.iloc[:5,:]
    d = d.reset_index()
    d = d.drop(columns=['index'])   
    mess = str(d['account_display_name'][0])+':'+str(d['message'][0])+':'+str(' '+ str(d[c][0])) +str(' '+c)    
    message['created_at'].append(a)
    message['series'].append(c)
    message['Top_post'].append(mess)
    message['N'].append(d[c][0])
dfmess = pd.DataFrame.from_dict(message)
dfmess = dfmess[['Top_post','N']]
newdf3 = pd.DataFrame()
newdf3 = pd.concat([newdf2, dfmess], axis=1, join="inner")
newdf3
#test วันเกิดเหตุการณ์ไม่ปกติ
newdf3.loc[(newdf3['created_at'] >= '2020-02-09')& (newdf3['created_at'] < '2020-02-10')]

#Merge DataFrame for heatmap
newdf4= pd.DataFrame()
newdf5 = pd.DataFrame()
newdf5 = newdf3['Top_post']
newdf4 = newdf2.join(newdf5,how='left')
newdf4.loc[(newdf4['created_at'] >= '2020-02-09')& (newdf4['created_at'] < '2020-02-10')]

#Facebook Reactions Normalized Chart 
import altair as alt
from vega_datasets import data
domain = ['angry','haha','love','sad','wow']
range_ = ['#d62728','#fdae6b', '#e377c2', '#636363',  '#6baed6']

source = newdf2

# print(source)
area1 = alt.Chart(source,title="Facebook Reactions 2020 Normalized Chart  ").mark_area().encode(
    x=alt.X("monthdate(created_at):T",title="Month-Date"),
    y=alt.Y("sum(count):Q", stack="normalize",title="Sum of Reactions"),
    color=alt.Color('series:N', scale=alt.Scale(domain=domain, range=range_),title='reactions')
).properties(width=1000,height=300).interactive()
area1

#heatmap + tootip show toppost 
import altair as alt
from vega_datasets import data
from datetime import datetime
import calendar


source = newdf4
# source
alt.Chart(
    source,
    title="Facebook Reactions"
).mark_rect().encode(
    x='monthdate(created_at):O',
    y='series:N',
    color=alt.Color('sum(count):Q', scale=alt.Scale(scheme="yelloworangered")),
    tooltip=[
        alt.Tooltip('monthdate(created_at):T', title='Date'),
        alt.Tooltip('sum(count):Q', title='Count'),
        alt.Tooltip('Top_post:N', title='Toppost')
    ]
).interactive().properties(width=600,height=200,title="Reaction")

#heatmap + tootip show toppost + selection time + area chart
import pandas as pd
import altair as alt
import calendar
listmonth = list(calendar.month_name)[1:13]
listmonth

base = alt.Chart(
    source,
    title="Facebook Reactions"
).mark_rect(stroke='black',strokeWidth=0.5).encode(
    x=alt.X('monthdate(created_at):T',title='Date'),
    y=alt.Y('series:N',title='Reactions'),
    color=alt.Color('sum(count):Q',scale=alt.Scale(scheme='yelloworangered'),title='Sum of reactions'),
    tooltip=[alt.Tooltip('monthdate(created_at):T',title='เดือน'), alt.Tooltip('sum(count):Q',title='Sum of reaction'),alt.Tooltip('Top_post:N')]
)
dropdown = listmonth
month_dropdown = alt.binding_select(options=dropdown)
month_select = alt.selection_single(fields=['month'], bind=month_dropdown,name="Month", init={'month': 'January'})

filter_month = base.add_selection(
    month_select
).transform_filter(
    month_select
).properties(width=800,height=200,title="Monthly Facebook Reactions ")

reactionchart = (area1) & (filter_month)
reactionchart.save('reactionchart_heatmap.html')
reactionchart

#test count word แทน top post
test = dffb.loc[(dffb['created_at'] >= '2020-02-09')& (dffb['created_at'] < '2020-02-10')]
test = test[['account_display_name','created_at','message','sad','comment','share','engagement']].sort_values(by='sad',ascending=False)
test = test.head(100)
test

#cleaning message
df = test['message']
df = df.to_frame()
def cleanText(text):
  # text = re.sub('[^ก-๙]','',text)
  stop_word = list(thai_stopwords())
  sentence = word_tokenize(text,engine="multi_cut")
  result = [word for word in sentence if word not in stop_word and " " not in word]
  return text
cleaning = []
for txt in df["message"]:
    cleaning.append(cleanText(txt))
cleaning[:10]

#cleaning message
df['cleaning'] = cleaning
df

#cleaning message + ลบคำที่ไม่มีความหมาย
def cleanText(text):
    text = str(text)
    text = re.sub('[^ก-๙]','',text)
    stop_word = list(thai_stopwords())+['คลิก','อี','จัน','ข่าว','เพจ','ลิงค์','บาท','ร้าน']
    sentence = word_tokenize(text)
    # sentence = word_tokenize(text)
    result = [word for word in sentence if word not in stop_word and " " not in word]
    return ",".join(result)

def tokenize(d):  
    result = d.split(",")
    result = list(filter(None, result))
    return result

new_text = []
for txt in df['cleaning']:
    new_text.append(cleanText(txt))


vectorizer = CountVectorizer(tokenizer=tokenize)
transformed_data = vectorizer.fit_transform(new_text)
count_data = zip(vectorizer.get_feature_names(), np.ravel(transformed_data.sum(axis=0)))
keyword_df = pd.DataFrame(columns = ['word', 'count'])
keyword_df['word'] = vectorizer.get_feature_names()
keyword_df['count'] = np.ravel(transformed_data.sum(axis=0))   
keyword_df.sort_values(by=['count'], ascending=False).head(10)
#สรุป การเลือก top post ในวันนั้นให้ความหมายดีกว่าการนับคำ เพราะโมเดลมักนับคำที่ไม่มีความหมายมาด้วย มีบางกรณีเท่านั้นที่เห็นประเด็นขัดเจน

#ลองเปลี่ยน normalized stacked area เป็น Layered Area Chart 
import altair as alt
from vega_datasets import data

source = newdf2
print(source)
alt.Chart(source).mark_area(opacity=0.3).encode(
    x="monthdate(created_at):T",
    y=alt.Y("sum(count):Q", stack=None),
    color="series:N"
).interactive()